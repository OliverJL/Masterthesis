\chapter{NIST Special Publication 800-90B}
\label{chap:NIST800-90B}
\section{General}

Topic of this thesis is an analysis of a Linux Random Number Generator's outcome delivered to kernel consumers, as introduced in \ref{sub:get-rnd-int-long}. Verification and estimation of entropy is 
a very difficult task, since there is no explicit definition for randomness. Hence, for a series of values generated by an RNG, randomness i.e. a sufficient degree of entropy is admitted, if the absence of any discernible pattern within a generated record can be diagnosed.
To assess the outcome and reliability of random number generating devices or implementations, several frameworks have been published, providing recommendations regarding the solid construction and validation of PRNG and TRNG sources. These also encompass a set of statistical tests and guidance how to conduct a profound verification (see \cite{robert2006dieharder, rukhin2010nist, turan2018nist}).
The National Institute of Standards and Technology (NIST) publishes a series of documents that describe U.S. federal government computer security policies, procedures and guidelines called NIST 800 series (see \cite[]{nist800pub}). To implement the analysis of this thesis, two Special Publications on the subject of validation and assessment of entropy and random number generation has been chosen:

\begin{table}[H]
	\centering
	\label{tab:nist800sps}
	\begin{tabular}{lll}\\ \hline 
		Number & Title & Release Date \\ \hline \hline 
		800-90B & \makecell[l]{Recommendation for the Entropy Sources Used for\\Random Bit Generation} & 1/10/2018 \\ \hline 
		800-22 Rev. 1a & \makecell[l]{A Statistical Test Suite for Random and Pseudorandom\\Number Generators for Cryptographic Applications} & 4/30/2010 \\ \hline 
	\end{tabular}
	\caption{Applied NIST Special Publications of the 800 Series (Status Final)}
\end{table} 


For this analysis, a framework based on 'Recommendation for the Entropy Sources Used for Random Bit Generation, NIST Special Publication 800- 90B' \cite{turan2018nist} has been chosen, able to deliver
usable guidance that will give conservative estimates on the amount of entropy in an entropy
source \cite{turan2015random}. The final version of the framework has been published in january 2018 and thus is very up to date. Beside that this decision has been met, since not just the outcome of and RNG can be assessed but also it's input. As mentioned in \ref{sec:int-rnd}, the Linux RNG executed on a Xen virtualized machine is not able to obtain noise from most designated sources. In fact, interrupt noise is the only input delivered during the initialization phase. Hence, to achieve a comprehensive analysis, an assessment of this input is considered to be required.\\
'NIST Special Publication (SP) 800-90' (a series consisting of three documents) specifies how to design an test 

 is all about
generating random numbers for cryptography. In SP 800-90, this is a two-stage process: first, an
entropy source provides an impossible-to-guess seed. Then, a deterministic cryptographic
algorithm (called a DRBG--deterministic random bit generator--in SP 800-90) expands the seed
into a long sequence of values that may be safely used for keys, IVs, nonces, etc.
\cite{turan2015random}.



 When assessing entropy by the second part of the series, 'NIST Special Publication 800-90B' (in the following referred to as NSP800-90B), an estimation can be achieved via 
two different tracks. Depending on an assumption regarding an entropy source, which has to be substantiated, the IID i.e. the Non-IID track should be used to analyze assumed random input 
\cite{turan2018nist}. The outcome of an analysis is an estimation of entropy, ascertained by statistical tests expressed in \textit{bits of entropy}. In the following sections, an introduction to these procedures, required to comprehend the results in TODO[Evaludation] is given.



%-----------------------------------------------
%NIST Special Publication (SP) 800-90 (a series consisting of three documents) is all about
%generating random numbers for cryptography. In SP 800-90, this is a two-stage process: first, an
%entropy source provides an impossible-to-guess seed. Then, a deterministic cryptographic
%algorithm (called a DRBG--deterministic random bit generator--in SP 800-90) expands the seed
%into a long sequence of values that may be safely used for keys, IVs, nonces, etc.
%\cite{turan2015random}

\section{Definitions \& Terminology}

\subsection{Distinction: Noise / Entropy / Randomness / RNG}
In literature, terms describing the in- and output of RNGs are sometimes used in a generic and partially confusing manner. For this thesis, the following terminology will be used, intending to distinguish regarding the maturity, source and purpose of (pseudo) random numbers.

\begin{itemize}
	\item \textbf{Entropy} Entropy is a property of a variable or value describing its certain degree of unpredictability. 
	\item \textbf{Noise} Noise is the input passed to an RNG or the state of an intermediate processing stage.
	While there might be a wide range in maturity, in general, noise is not assumed to provide a cryptographic safe degree of entropy. Since the Linux RNG enriches entropy via various steps (see \ref{fig:linux-prng-arch}) depending on the stage \textit{Noise} will be subdivided into:
	\begin{itemize}
		\item \textbf{Raw Noise} \textit{Noise} sources of the Linux RNG, delivering external physical states like interrupts (see \ref{sec:inp-src-lprng}).
		\item \textbf{Pre-processed Noise} Raw Noise being pre-processed via the fast-pool, assumed to have a poor level of \textit{Entropy} (see \ref{sub:fast-pool}).
		\item \textbf{Conditioned Noise} \textit{Noise} prepared by a conditioning component and hence providing an advanced level of \textit{Entropy}, like the input-pool (see \ref{sub:input-pool}).	
	\end{itemize}
	\item \textbf{Randomness} The outcome of an RNG delivered to a user mode or kernel consumer. This output is assumed to provide a cryptographic safe degree of \textit{Entropy}.
	\item \textbf{Entropy Source} The generic term for any kind of sources dedicated to deliver values assumed to provide a certain degree of \textit{Entropy}. This includes all kind of \textit{Noise Sources} as well as \textit{Random Sources}.	
	\item \textbf{RBG/RNG} NIST 800 uses the more generalizing term \textit{RBG (Random Bit Generator)} instead of \textit{RNG (Random Number Generator)}. Since this thesis relates to Linux x86 Kernel where random numbers are always obtained and applied on at least byte level, the term \textit{RNG} will be used.
\end{itemize}

\subsection{The formal Definition of Entropy}
In Information theory, for a set of information units, entropy in generall describes the level of disorder i.e. the unpredictability of each single unit's state at a given alphabet. Various formal definitions of entropy exist \cite{hagerty2012entropy}, while the definition of Shannon (see \ref{fig:form-entropy-shan}) is quite common for a binary alphabet. 

\begin{figure}[H]
	\begin{align*}
	\displaystyle H(X) := \sum_{s \in {0,1}} I(p_s) p_s = \sum_{s \in {0,1}} -\lg_2 \frac{1}{p_s} p_s && \text{$p_s$ = P(X=s)}
	\end{align*}
	\caption{Formal definiton of entropy by Shannon for a binary alphabet}
	\label{fig:form-entropy-shan}
\end{figure}
According to Shannon, entropy is defined as the product of self-information \textit{I} multiplied by the expected value. For a binary alphabet, it can be measured in unit \textit{bits}. 



\section{NIST Special Publication 800-90 B}

The NIST Special Publication (SP) 800-90 series of recommendations provides 
guidance on the validation and construction of Pseudo- and Non-deterministic Random Number Generators that can be used for cryptographic secure applications. More precisely, the recommendation specifies how to design and validate noise delivering sources that are used as input for RNGs. Transfered to the Linux RNG this verification approach can be applied to the raw sources described in \ref{sec:inp-src-lprng} as well as on conditioned or pre-processed noise of the input- i.e. fast-pool (\ref{sub:input-pool}, \ref{sub:fast-pool}). The series consist of three parts. NSP800-90A and NSP800-90C refer to the construction of RNG mechanisms, while NSP800-90B addresses the analysis and validation of sources delivering noise. A source that conforms to this recommendation can be used by RNGs to produce a sequence of random values \cite{rukhin2010nist} i.e. randomness.\\
The validation process described by NSP800-90B consist of several sequential and iterative steps. For each step, a number of prerequisites and assumption is defined. A step can be a task required to prepare the next step or a test procedure consisting of several statistical test. In general, the validation process is all about determining entropy estimates, which are the outcome of particular test procedures applied on recorded noise datasets. Traditional statistical procedures like applied by NSP800-22 would fail, since this framework is testing under the hypothesis of unbiased, independent distributed values. The statistical test procedures of NSP800-90B are aware that noise sources will most likely not deliver entropy at a high level and hence assess accordingly. Figure \ref{fig-entropy-est-strategy} gives a overview of the entire Entropy Estimation Strategy of NSP800-90B. The following sections will explain relevant components of this strategy more detailed.

 \begin{figure}[H]
 	\centering
 	\includegraphics[scale=0.6]{img/nsp800-90b-entropy-est-strategy.png}
 	\caption{Entropy Estimation Strategy according to NIST SP 800-90B (from \cite{turan2018nist}, references illustrated refer to the origin publication)}
 	\label{fig-entropy-est-strategy}
 \end{figure}

\subsection{Min-Entropy}\label{sub:min-entropy}
In NSP800-90B, statistical test are bundled to test procedures. Each single test within a procedure delivers two related values, $p_{max}$ and min-entropy. The min-entropy of an independent discrete random variable $X$
that takes values from the set $A$ with probability $p_i$ is defined as $H$.

\begin{equation}
A = \{x_1, x_2, x_3, ..., x_k\}
\label{eq:min-ent-A}
\end{equation}
\begin{equation}
Pr(X=x_i) = p_i 
\Domain{i=\{1,...,k\}}
\end{equation}
\begin{equation}
H = \underset{1\leq i\leq k}{\mathrm{min}}(-\log_2p_i) = -\log_2 \underset{1\leq i\leq k}{\mathrm{max}}p_i
\end{equation}

If $X$ has min-entropy $H$, then the probability $p_{max}$ of observing any particular value for $X$ is $\le$ $2^{-H}$ \cite{turan2018nist}. The outcome of a test procedure is a single min-entropy value, representing the lowest min-entropy delivered by a specific statistical test.

\subsection{Data Records}\label{sub:data-records}
The test procedures of NSP800-90B are applied on data sets resulting from recording sequential produced values from the noise sources to be analyzed. A data record should at least provide 1.000.000 samples directly obtained from the source. If this number can not be achieved, which is the case for the interrupt noise source analyzed in this thesis, several smaller sets may be concatenated, while each set shall contain at least 1000 samples. In total the data record shall finally contain 1.000.000 samples \cite{turan2018nist}. A downstream test procedure, required to be conducted for a comprehensive result is the \textit{restart test}. This procedure requires to record the value sequences delivered by a noise source 1000 times. For each restart, 1000 samples shall be tracked. The recorded data is merged to a single binary data file, the restart matrix. 
NSP800-90B does not require i.e. recommend to pass the recorded (merged) data directly to the test procedures. After recording values from a noise source, data may be vetted. If it is obvious, also by the nature of a an analyzed noise source, that there might be a variation in a fixed pattern regarding the obtained values, those may be trimmed at a fix pattern. Though it is essential that the sequence is kept, i.e. the trimmed values are aligned in the same order as delivered by the noise source. The standard requires to document and advocate this approach, since the analysis result is reduced to those bytes. If a noise source f.e. delivers four byte values based on simple incrementation, it is obvious that the higher bytes of each value in the sequence will have less variation than the lower ones. In this case, trimming is recommended.

\subsection{Determining the Track - IID vs. Non-IID}\label{sub:iid-noniid-track}
When conducting an analysis by NSP800-90B a fundamental assumption to the \textit{independency and identically distribution} of the analyzed noise source has to be delivered. Subsequently one of the two tracks illustrated in \ref{fig-entropy-est-strategy} needs to be chosen to proceed: The non-IID- i.e. the IID-track. Both tracks basically deviate regarding the assumed maturity of entropy a noise source is able to deliver. An independent and identically distributed noise source is characterized by the following properties: 
\begin{itemize}
	\item \textit{Independence} All values obtained by the noise source have no dependency to each other.
		\item \textit{Identically Distribution} All series of values obtained by a noise source fit within the same distribution.
\end{itemize}
As any pre-processing i.e. conditioning components of an RNG are deterministic algorithms, a noise source having IID properties is desirable i.e. essential to ensure reliable random output. Hence, a proper classification regarding the maturity of 'random' noise is required. The recommendation supposes an IID providing source which needs to be proven. If the the prerequisites of IID can not be confirmed, the Non-IID track has to be applied. To reach a decision, NSP800-90B defines three criteria which must be complied in order to take the IID-track.

\begin{enumerate}
	\item \textit{Noise Source Design} A fundamental IID claim needs to be stated i.e. to be rejected, based on the design of the noise source. In both cases the assumption regrading IID i.e. Non-IID needs to be documented. If IID is assumed, the appropriate test procedure should be applied.
	\item \textit{Sequential IID-Test Pass} The test procedure dedicated to verify the IID assumption on the sequential dataset confirms an acceptable degree of independency and identically distribution.
	\item \textit{Restart IID-Test Pass} The test procedure dedicated to verify the IID assumption on the restart datatset (1000 samples of 1000 restarts) confirms an acceptable degree of independency and identically distribution.
\end{enumerate}

If validation doesn't fail, the test procedure will deliver an entropy estimate which will be used to parametrize the next iteration of the appropriate test procedure. If the IID track failed, the Non-IID track needs to be applied. If this also fails, the data record is assessed to provide not sufficient entropy at all.\\
To anticipate the analysis in SEC TODO, the IID-assumption had to be rejected for noise delivered by the interrupt sources. Consequently the Non-IID test procedure has been applied and thus will be introduced briefly in the following section.

\subsection{The Non-IID test procedure according to NIST 900-20B}\label{sub:non-iid-test-proc}
\setcounter{secnumdepth}{3}
If a noise source is delivering values confirming the IID-assumption min-entropy estimation is determined by the \textit{Most Common Value Estimate} procedure. The verification process for Non-IID values investigates into several directions and thus is more complex. In total the procedures are applied. An overview is given in \ref{non-iid-est:tab-overview}. Each procedure calculates a min-entropy estimate describing the minimum entropy each single unit, in this case byte, can be confirmed. The minimum of each estimates is finally delivered as the outcome for the entire test procedure. 
 
\begin{table}[H]
	\centering
	\begin{tabular}{rll}
		\hline \hline
		(I.)& Most Common Value Estimate & see \ref{non-iid-est:mcve} \\ 
		(II.)& Collision Estimate & see \ref{non-iid-est:colle} \\ 
		(III.)& Markov Estimate & see \ref{non-iid-est:marke} \\ 
		(IV.)& Compression Estimate & see \ref{non-iid-est:compe} \\ 
		(V.)& t-Tuple Estimate & see \ref{non-iid-est:tte} \\ 
		(VI.)& LRS Estimate & see \ref{non-iid-est:lrs} \\ 
		(VII.)& Multi Most Common in Window Prediction Estimate & see \ref{non-iid-est:mmcwpe} \\ 
		(VIII.)& Lag Prediction Estimate & see \ref{non-iid-est:lpe} \\ 
		(IX.)& MultiMMC Prediction Estimate & see \ref{non-iid-est:mmcpe} \\ 
		(X.)& LZ78Y Prediction Estimate & see \ref{non-iid-est:lz78y} \\ \hline \hline
	\end{tabular}
	\label{non-iid-est:tab-overview}
	\caption{Overview of non-iid entropy estimation tests recommended in NIST 900-20B}
\end{table}

\subsubsection{Most Common Value Estimate}\label{non-iid-est:mcve}
This procedure consists of two steps:
\begin{enumerate}
	\item Determine the proportion of the most common value in the entire dataset.
	\item Construct a confidence interval for this proportion.
\end{enumerate}
The min-entropy is determined by the upper bound of this confidence interval.

\subsubsection{Collision Estimate}\label{non-iid-est:colle}
This procedure estimates the probability of the most-likely output value regrading collision times. In this context, a collision is defined as a repetition of any value when conducting a binary search. The estimate is described be the mean number of samples to the first collision in the dataset. For a noise source having a bias toward a particular output, the mean distance to the occurrence of a collision will be short and hence deliver a low min-entropy estimate. 

\subsubsection{Markov Estimate}\label{non-iid-est:marke}
To determine the min-entropy an n$^{th}$-order Markov model is applied on bit sequences obtained from the noise source. Thereby, the value for each bit is compared to the predicted value based to the previous $n$ bits within the sequence. The min-entropy is determined by the highest probability $\hat{p}$ at fixed sequence length $L$:

\begin{equation}
entropy_{min} = min(\frac{-log_2(\hat{p})}{L})
%\Domain{\hat{p} : highest probability, L : sequence length}
\end{equation}

\subsubsection{Compression Estimate}\label{non-iid-est:compe}
The Compression Estimate determines the min-entropy by investigating how strong the dataset can be compressed. To do so, two disjunct value groups are build: The dictionary and the test group. The average number of samples required delivers the mean value is applied identically to the collision estimate procedure to calculate the min-entropy.

\subsubsection{t-Tuple Estimate}\label{non-iid-est:tte}
This procedure examines the frequency of $t$-tuples (pairs, triples, ...) occurring in the dataset, while tuples can overlap. The algorithm determines the tuple having maximum length $t_{max}$ and occurring at least 35 times. For all tuples of length $\le t_{max}$ an estimate on the maximum individual sample value probability $p_{t} ... p_{1}$ is ascertained by which the most common value $p_u$ can be obtained as following ($L$ corresponds to the number pf samples, constant 2.576 to a confidence interval of 99\%):
\begin{equation}
p_u = min\left(1,\hat{p}_{max} + 2.576 \sqrt{\frac{\hat{p}_{max}(1-\hat{p}_{max})}{L-1}}\right)
\end{equation}
The min-entropy estimation is finally determined by $-log_2(p_u)$.

\subsubsection{Longest Repeated Estimate}\label{non-iid-est:lrs}
This procedure is a complementary estimate to the $t$-tuple estimate, which is not able to handle large tuple sizes. To successfully determine the Longest Repeated Estimate the dataset need to provide repeating patterns with the following properties:
\begin{itemize}
	\item $u:$ The smallest length of all tuples occurring less then 35 times in the dataset.
	\item $v:$ The largest length of all tuples occurring at least twice in the dataset.
\end{itemize}
If $v < u$ the estimate can not computed. Otherwise a range $W$ is defined as $W = {u,..,v}$.
\begin{equation}
	P_w = \frac{\displaystyle\sum_{i=u}^{v}\binom{C_i}{2}}{\binom{L-W+1}{2}}
\end{equation}
The estimated average collision probability per string symbol is calculated as $P_{max,W} = P_{W}^{\frac{1}{W}}$, while $C_i$ is the number of occurrences of the $i^{th}$ value in $W$. 
$\hat{p}_{max}$ is the maximum value i.e. $max(P_{max,u}...P_{max,v})$ which is used to determine $p_u$ and finally the min-entropy equivalent to the t-Tuple estimation procedure (see \ref{non-iid-est:tte}).

\subsubsection{Multi Most Common in Window Prediction Estimate}\label{non-iid-est:mmcwpe}
The Multi Most Common in Window Prediction Estimate (MultiMCW Prediction Estimate) addresses datasets in which the most common value changes over time, but remains relatively stationary over a reasonable length. To determine the min-entropy, several subpredictors are applied. Each subpredictor is used to guess the next value in the dataset based on the last $w$ values, while the length of $w$ varies. The MultiMCW predictor maintains an internal scoreboard, recording whether a prediction was correct. If prediction collides, i.e. for the next prediction, several values have the same probability, the most common sample value that has appeared most recently is chosen. For each subpredictor, a global $P_{global}$ and local $P_{local}$ performance value is calculated.
The min-entropy of the MultiMCW Prediction Estimate procedure is finally the negative logarithm of the greater performance metric:
\begin{equation}
entropy_{min} = -log_2\left(max\left(P'_{global}, P'_{local}, \frac{1}{k}\right)\right)
\end{equation}

\subsubsection{Lag Prediction Estimate}\label{non-iid-est:lpe}
Similar to the MultiMCW Prediction Estimate, the Lag Prediction Estimate procedure uses several subpredictors predicting the next value in the dataset, depending on a specific lag. Also in this case, an internal scoreboard tracks whether prediction was correct. Based on a global $P_{global}$ and local $P_{local}$ performance value, the min-entropy is calculated identical to the MultiMCW Prediction Estimate procedure. 

\subsubsection{Multiple Markov Model with Counting Prediction Estimate}\label{non-iid-est:mmcpe}
In a typical Markov model the probability of a transition from one state to another is tracked to predict the next output. Contrary in the Multiple MMC Prediction Estimate procedure, the observed frequencies of transitions from one value to the next are recorded. Prediction for the next output is made by several subpredictors, based on the most frequently observed transition from the current output. Each subpredictors has a specific depth which is defined by their total number. The first one has depth 1, the n$h$ subpredictor depth n. Also in this case, min-entropy is determined by the global $P_{global}$ and local $P_{local}$ performance, tracked via an internal scoreboard analogues to the MultiMCW Prediction Estimate procedure (see \ref{non-iid-est:mmcwpe}).

\subsubsection{LZ78Y Prediction Estimate}\label{non-iid-est:lz78y}
LZ78 is a dictionary based compression algorithm. Each subsequent string in a dataset is added to the dictionary if not contained. Otherwise the position is returned. 


\subsection{NIST 800-22}

standard normal and the chi-square 
\subsubsection{Frequency (Monobit) Test}\label{ss:freq-monobit}
\begin{itemize}
	\item \textbf{Test Domain}
	On bitlevel, the Frequency (Monobit) Test analysis whether the proportion of zeros and ones are approximately equal within the entire dataset. It is recommended that this test is run first, since it supplies most basic evidence for the presence of non-randomness in the dataset: non-uniformity. If the test fails, it is most likely that other tests will fail as well. 
	\item \textbf{Reference Distribution}
	\item \textbf{Test statistic}	
\end{itemize}
\subsubsection{Frequency Test within a Block}
\begin{itemize}
	\item \textbf{Test Domain}
\end{itemize}


Similar to the Frequency (Monobit) Test, the Frequency Test within a Block checks the proportion of zeros and ones within serveral M-bit block extracted from the dataset. 
\subsubsection{Runs Test}
A run is defined as a sequence of subequent bits havinn the same value. An assumed characteristic for randomness is that the oscillation of bits should neither occur to fast, nor to slow. Internally the Runs Test conducts a Frequency Monobit Test at first. If it failes, $p$ is set to 0.
\subsubsection{Tests for the Longest-Run-of-Ones in a Block}
\subsubsection{Binary Matrix Rank Test}
\subsubsection{Discrete Fourier Transform (Spectral) Test}
\subsubsection{Non-overlapping Template Matching Test}
\subsubsection{Overlapping Template Matching Test}
\subsubsection{Maurer's "Universal Statistical" Test}
\subsubsection{Linear Complexity Test}
\subsubsection{Serial Test}
\subsubsection{Approximate Entropy Test}
\subsubsection{Cumulative Sums (Cusums) Test}
\subsubsection{Random Excursions Test}
\subsubsection{Random Excursions Variant Test}



%The values delivered by an noise source able to provide IID values

%\cite{gnedenko1954independent}

%Independent and identically Distribution
%
% sdfsdf
% \gls{python}
%Independent and identically distributed random variables
% the lowest  

%  \ref{sub:min-entropy}

% taking into account the
% expected statistical 
%
%the noise source will not produce unbiased, independent binary data
%
%NIST SP 800-22
%
%Since, in most cases,
%the noise source will not produce unbiased, independent binary data, traditional statistical
%procedures (e.g., the randomness tests described in NIST SP 800-22) that test the hypothesis of
%unbiased, independent bits will almost always fail, and thus are not useful for monitoring the noise
%source. In general, tests on the noise source need to be tailored carefully, taking into account the
%expected statistical behavior of the correctly operating noise source.












